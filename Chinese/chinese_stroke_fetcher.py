#!/usr/bin/env python3
"""
Chinese Character Stroke Data Fetcher - GitHub Version
Fetches stroke order data directly from the HanziWriter GitHub repository.
This version is more reliable as it accesses the raw data files directly.
"""

import json
import time
import requests
from typing import Dict, List, Optional

# 100 most common characters for children learning Chinese
BASIC_CHARACTERS = [
    # Numbers 1-10
    '‰∏Ä', '‰∫å', '‰∏â', 'Âõõ', '‰∫î', 'ÂÖ≠', '‰∏É', 'ÂÖ´', '‰πù', 'ÂçÅ',
    # Common characters (family, daily life, nature)
    '‰∫∫', 'Âè£', 'Êâã', 'Êó•', 'Êúà', 'Ê∞¥', 'ÁÅ´', 'Êú®', 'Èáë', 'Âúü',
    'Â§ß', 'Â∞è', '‰∏≠', '‰∏ä', '‰∏ã', 'Â∑¶', 'Âè≥', 'Â§©', 'Âú∞', 'Â±±',
    'Áî∞', 'Áü≥', 'ÁõÆ', 'ËÄ≥', 'ÂøÉ', 'Èó®', 'Â•≥', 'Â≠ê', 'È©¨', 'Áâõ',
    'Áæä', 'È∏ü', 'È±º', 'Á±≥', 'Á´π', '‰∏ù', 'Ëô´', 'Ë¥ù', 'ËßÅ', 'ËΩ¶',
    'È£é', '‰∫ë', 'Èõ®', 'Èõ™', 'Áîµ', 'ÂàÄ', 'Âäõ', 'Âèà', 'Êñá', 'Êñπ',
    # Common verbs and adjectives
    '‰∏ç', '‰πü', '‰∫Ü', 'Âú®', 'Êúâ', 'Êàë', '‰Ω†', '‰ªñ', 'Â•π', 'Â•Ω',
    'Êù•', 'Âéª', 'Âá∫', 'ÂÖ•', 'Êú¨', 'ÁôΩ', 'Á∫¢', 'Èïø', 'Â§ö', 'Â∞ë',
    'È´ò', 'ÂºÄ', 'Áîü', 'Â≠¶', 'Â∑•', 'Áî®', 'Ëµ∞', 'È£û', 'ÂêÉ', 'Âñù',
    'Áúã', 'Âê¨', 'ËØ¥', 'ËØª', 'ÂÜô', 'Âùê', 'Á´ô', 'Áà±', 'Á¨ë', 'Âì≠'
]


def fetch_from_github_raw(character: str) -> Optional[Dict]:
    """
    Fetch directly from GitHub raw.
    Repository: https://github.com/chanind/hanzi-writer-data
    Files use the actual character in the filename, not hex codes!
    """
    try:
        # URL uses the actual character, not hex!
        # Example: https://raw.githubusercontent.com/.../data/‰∫∫.json
        url = f"https://raw.githubusercontent.com/chanind/hanzi-writer-data/refs/heads/master/data/{character}.json"
        
        response = requests.get(url, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            unicode_hex = format(ord(character), '05x')
            return {
                'character': character,
                'unicode': unicode_hex,
                'stroke_count': len(data.get('strokes', [])),
                'strokes': data.get('strokes', []),
                'medians': data.get('medians', []),
                'radical': data.get('radical', ''),
            }
        
        return None
            
    except Exception as e:
        return None


def download_full_dataset() -> Dict[str, Dict]:
    """
    Alternative: Download characters one by one and build a dataset.
    The hanzi-writer-data repo doesn't have a single combined file,
    so we'll fetch each character individually but cache them.
    """
    print("Note: Hanzi-writer-data doesn't provide a combined dataset file.")
    print("Fetching characters individually instead...\n")
    return {}


def extract_characters_from_dataset(dataset: Dict, characters: List[str]) -> List[Dict]:
    """
    Extract specific characters from the full dataset.
    """
    results = []
    
    print(f"\nExtracting {len(characters)} characters from dataset...")
    print("=" * 60)
    
    for i, char in enumerate(characters, 1):
        print(f"[{i}/{len(characters)}] Processing {char}...", end='')
        
        if char in dataset:
            data = dataset[char]
            result = {
                'character': char,
                'unicode': format(ord(char), 'x'),
                'stroke_count': len(data.get('strokes', [])),
                'strokes': data.get('strokes', []),
                'medians': data.get('medians', []),
                'radical': data.get('radical', ''),
            }
            results.append(result)
            print(f" ‚úì ({result['stroke_count']} strokes)")
        else:
            print(f" ‚úó Not found in dataset")
    
    print("=" * 60)
    print(f"Successfully extracted {len(results)} out of {len(characters)} characters")
    
    return results


def fetch_all_characters_individually(characters: List[str], delay: float = 0.5) -> List[Dict]:
    """
    Fetch stroke data for all characters one by one.
    Uses longer delay to avoid rate limiting.
    """
    results = []
    total = len(characters)
    
    print(f"Fetching stroke data for {total} characters individually...")
    print("=" * 60)
    
    for i, char in enumerate(characters, 1):
        print(f"[{i}/{total}] Fetching {char}...", end='')
        
        data = fetch_from_github_raw(char)
        
        if data:
            results.append(data)
            print(f" ‚úì ({data['stroke_count']} strokes)")
        else:
            print(f" ‚úó Failed")
        
        # Be very respectful to the API - longer delay
        if i < total:
            time.sleep(delay)
    
    print("=" * 60)
    print(f"Successfully fetched {len(results)} out of {total} characters")
    
    return results


def create_embedded_dataset() -> List[Dict]:
    """
    Fallback: Create a dataset with embedded data for common characters.
    This ensures the script always produces useful output.
    
    Note: This contains simplified stroke data. For production use,
    download the full dataset from HanziWriter when you have internet access.
    """
    print("Using embedded fallback dataset...")
    print("‚ö†Ô∏è  Note: This is simplified stroke data for offline use.")
    print("    For accurate stroke data, run without --embedded when online.\n")
    
    # Embedded data for essential characters (simplified representations)
    # Stroke counts are accurate, but paths are simplified
    embedded = {
        # Numbers
        '‰∏Ä': {'count': 1, 'radical': '‰∏Ä'},
        '‰∫å': {'count': 2, 'radical': '‰∏Ä'},
        '‰∏â': {'count': 3, 'radical': '‰∏Ä'},
        'Âõõ': {'count': 5, 'radical': 'Âõó'},
        '‰∫î': {'count': 4, 'radical': '‰∫å'},
        'ÂÖ≠': {'count': 4, 'radical': 'ÂÖ´'},
        '‰∏É': {'count': 2, 'radical': '‰∏Ä'},
        'ÂÖ´': {'count': 2, 'radical': 'ÂÖ´'},
        '‰πù': {'count': 2, 'radical': '‰∏ø'},
        'ÂçÅ': {'count': 2, 'radical': 'ÂçÅ'},
        # Basic characters
        '‰∫∫': {'count': 2, 'radical': '‰∫∫'},
        'Âè£': {'count': 3, 'radical': 'Âè£'},
        'Êâã': {'count': 4, 'radical': 'Êâã'},
        'Êó•': {'count': 4, 'radical': 'Êó•'},
        'Êúà': {'count': 4, 'radical': 'Êúà'},
        'Ê∞¥': {'count': 4, 'radical': 'Ê∞¥'},
        'ÁÅ´': {'count': 4, 'radical': 'ÁÅ´'},
        'Êú®': {'count': 4, 'radical': 'Êú®'},
        'Èáë': {'count': 8, 'radical': 'Èáë'},
        'Âúü': {'count': 3, 'radical': 'Âúü'},
        'Â§ß': {'count': 3, 'radical': 'Â§ß'},
        'Â∞è': {'count': 3, 'radical': 'Â∞è'},
        '‰∏≠': {'count': 4, 'radical': '‰∏®'},
        '‰∏ä': {'count': 3, 'radical': '‰∏Ä'},
        '‰∏ã': {'count': 3, 'radical': '‰∏Ä'},
        'Â∑¶': {'count': 5, 'radical': 'Â∑•'},
        'Âè≥': {'count': 5, 'radical': 'Âè£'},
        'Â§©': {'count': 4, 'radical': 'Â§ß'},
        'Âú∞': {'count': 6, 'radical': 'Âúü'},
        'Â±±': {'count': 3, 'radical': 'Â±±'},
        'Áî∞': {'count': 5, 'radical': 'Áî∞'},
        'Áü≥': {'count': 5, 'radical': 'Áü≥'},
        'ÁõÆ': {'count': 5, 'radical': 'ÁõÆ'},
        'ËÄ≥': {'count': 6, 'radical': 'ËÄ≥'},
        'ÂøÉ': {'count': 4, 'radical': 'ÂøÉ'},
        'Èó®': {'count': 3, 'radical': 'Èó®'},
        'Â•≥': {'count': 3, 'radical': 'Â•≥'},
        'Â≠ê': {'count': 3, 'radical': 'Â≠ê'},
        'È©¨': {'count': 3, 'radical': 'È©¨'},
        'Áâõ': {'count': 4, 'radical': 'Áâõ'},
        'Áæä': {'count': 6, 'radical': 'Áæä'},
        'È∏ü': {'count': 5, 'radical': 'È∏ü'},
        'È±º': {'count': 8, 'radical': 'È±º'},
        'Á±≥': {'count': 6, 'radical': 'Á±≥'},
        'Á´π': {'count': 6, 'radical': 'Á´π'},
        '‰∏ù': {'count': 5, 'radical': '‰∏Ä'},
        'Ëô´': {'count': 6, 'radical': 'Ëô´'},
        'Ë¥ù': {'count': 4, 'radical': 'Ë¥ù'},
        'ËßÅ': {'count': 4, 'radical': 'ËßÅ'},
        'ËΩ¶': {'count': 4, 'radical': 'ËΩ¶'},
        'È£é': {'count': 4, 'radical': 'È£é'},
        '‰∫ë': {'count': 4, 'radical': '‰∫å'},
        'Èõ®': {'count': 8, 'radical': 'Èõ®'},
        'Èõ™': {'count': 11, 'radical': 'Èõ®'},
        'Áîµ': {'count': 5, 'radical': 'Áî∞'},
        'ÂàÄ': {'count': 2, 'radical': 'ÂàÄ'},
        'Âäõ': {'count': 2, 'radical': 'Âäõ'},
        'Âèà': {'count': 2, 'radical': 'Âèà'},
        'Êñá': {'count': 4, 'radical': 'Êñá'},
        'Êñπ': {'count': 4, 'radical': 'Êñπ'},
        '‰∏ç': {'count': 4, 'radical': '‰∏Ä'},
        '‰πü': {'count': 3, 'radical': '‰πô'},
        '‰∫Ü': {'count': 2, 'radical': '‰πô'},
        'Âú®': {'count': 6, 'radical': 'Âúü'},
        'Êúâ': {'count': 6, 'radical': 'Êúà'},
        'Êàë': {'count': 7, 'radical': 'Êàà'},
        '‰Ω†': {'count': 7, 'radical': '‰∫∫'},
        '‰ªñ': {'count': 5, 'radical': '‰∫∫'},
        'Â•π': {'count': 6, 'radical': 'Â•≥'},
        'Â•Ω': {'count': 6, 'radical': 'Â•≥'},
        'Êù•': {'count': 7, 'radical': 'Êú®'},
        'Âéª': {'count': 5, 'radical': 'Âé∂'},
        'Âá∫': {'count': 5, 'radical': 'Âáµ'},
        'ÂÖ•': {'count': 2, 'radical': 'ÂÖ•'},
        'Êú¨': {'count': 5, 'radical': 'Êú®'},
        'ÁôΩ': {'count': 5, 'radical': 'ÁôΩ'},
        'Á∫¢': {'count': 6, 'radical': 'Á∫ü'},
        'Èïø': {'count': 4, 'radical': 'Èïø'},
        'Â§ö': {'count': 6, 'radical': 'Â§ï'},
        'Â∞ë': {'count': 4, 'radical': 'Â∞è'},
        'È´ò': {'count': 10, 'radical': 'È´ò'},
        'ÂºÄ': {'count': 4, 'radical': '‰∏Ä'},
        'Áîü': {'count': 5, 'radical': 'Áîü'},
        'Â≠¶': {'count': 8, 'radical': 'Â≠ê'},
        'Â∑•': {'count': 3, 'radical': 'Â∑•'},
        'Áî®': {'count': 5, 'radical': 'Áî®'},
        'Ëµ∞': {'count': 7, 'radical': 'Ëµ∞'},
        'È£û': {'count': 3, 'radical': 'È£û'},
        'ÂêÉ': {'count': 6, 'radical': 'Âè£'},
        'Âñù': {'count': 12, 'radical': 'Âè£'},
        'Áúã': {'count': 9, 'radical': 'ÁõÆ'},
        'Âê¨': {'count': 7, 'radical': 'Âè£'},
        'ËØ¥': {'count': 9, 'radical': 'ËÆ†'},
        'ËØª': {'count': 10, 'radical': 'ËÆ†'},
        'ÂÜô': {'count': 5, 'radical': 'ÂÜñ'},
        'Âùê': {'count': 7, 'radical': 'Âúü'},
        'Á´ô': {'count': 10, 'radical': 'Á´ã'},
        'Áà±': {'count': 10, 'radical': 'Áà´'},
        'Á¨ë': {'count': 10, 'radical': 'Á´π'},
        'Âì≠': {'count': 10, 'radical': 'Âè£'},
    }
    
    # Generate simplified stroke paths based on stroke count
    def generate_simplified_strokes(count: int, char: str) -> tuple:
        """Generate simplified stroke data for demonstration."""
        strokes = []
        medians = []
        
        # Create simple horizontal or vertical strokes
        for i in range(count):
            y_pos = 200 + (600 // (count + 1)) * (i + 1)
            stroke = f"M 200 {y_pos} L 800 {y_pos}"
            median = [[200, y_pos], [800, y_pos]]
            strokes.append(stroke)
            medians.append(median)
        
        return strokes, medians
    
    results = []
    for char in BASIC_CHARACTERS:
        if char in embedded:
            data = embedded[char]
            strokes, medians = generate_simplified_strokes(data['count'], char)
            
            results.append({
                'character': char,
                'unicode': format(ord(char), 'x'),
                'stroke_count': data['count'],
                'strokes': strokes,
                'medians': medians,
                'radical': data['radical'],
            })
    
    print(f"Created dataset with {len(results)} characters")
    print(f"All 100 basic characters included with accurate stroke counts.\n")
    return results


def save_to_json(data: List[Dict], filename: str = "chinese_stroke_data.json"):
    """Save the collected data to a JSON file in the format expected by Swift."""
    try:
        # Convert array to dictionary with "U+XXXX" keys (Swift loader format)
        dict_data = {}
        for item in data:
            # Use "U+XXXX" format as key (uppercase, 4 digits minimum)
            unicode_int = int(item['unicode'], 16)
            key = f"U+{item['unicode'].upper().zfill(4)}"
            
            # Convert medians to the format Swift expects: array of stroke arrays
            # Each stroke is an array of {x, y, t} points
            strokes_data = []
            for stroke_idx, median_points in enumerate(item.get('medians', [])):
                stroke_points = []
                for point_idx, point in enumerate(median_points):
                    if len(point) >= 2:
                        # Calculate time value (evenly distributed across stroke)
                        t_value = point_idx / max(1, len(median_points) - 1) if len(median_points) > 1 else 0
                        stroke_points.append({
                            'x': point[0],
                            'y': point[1],
                            't': t_value
                        })
                if stroke_points:
                    strokes_data.append(stroke_points)
            
            dict_data[key] = {
                'character': item['character'],
                'codepoint': unicode_int,
                'strokes': strokes_data
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(dict_data, f, ensure_ascii=False, indent=2)
        print(f"\n‚úì Data saved to {filename}")
        print(f"   Format: Dictionary with {len(dict_data)} entries (Swift-compatible)")
        return True
    except Exception as e:
        print(f"\n‚úó Error saving file: {str(e)}")
        print(f"   Error details: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return False


def create_swift_compatible_format(data: List[Dict], filename: str = "stroke_data_swift.json"):
    """Create a Swift-friendly JSON format optimized for iOS apps."""
    swift_data = {
        "version": "1.0",
        "character_count": len(data),
        "characters": {}
    }
    
    for item in data:
        char = item['character']
        swift_data["characters"][char] = {
            "unicode": item['unicode'],
            "strokeCount": item['stroke_count'],
            "strokes": item['strokes'],
            "medians": item['medians'],
            "radical": item.get('radical', '')
        }
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(swift_data, f, ensure_ascii=False, indent=2)
        print(f"‚úì Swift-compatible data saved to {filename}")
        return True
    except Exception as e:
        print(f"‚úó Error saving Swift format: {str(e)}")
        return False


def create_summary_report(data: List[Dict]):
    """Create a summary report of the collected data."""
    if not data:
        print("\nNo data to summarize.")
        return
    
    total_chars = len(data)
    stroke_counts = [item['stroke_count'] for item in data]
    avg_strokes = sum(stroke_counts) / len(stroke_counts)
    min_strokes = min(stroke_counts)
    max_strokes = max(stroke_counts)
    
    print("\n" + "=" * 60)
    print("SUMMARY REPORT")
    print("=" * 60)
    print(f"Total characters collected: {total_chars}")
    print(f"Average stroke count: {avg_strokes:.1f}")
    print(f"Minimum strokes: {min_strokes}")
    print(f"Maximum strokes: {max_strokes}")
    print(f"\nCharacters by stroke count:")
    
    # Group by stroke count
    stroke_groups = {}
    for item in data:
        count = item['stroke_count']
        if count not in stroke_groups:
            stroke_groups[count] = []
        stroke_groups[count].append(item['character'])
    
    for count in sorted(stroke_groups.keys()):
        chars = ''.join(stroke_groups[count])
        print(f"  {count:2d} strokes: {chars} ({len(stroke_groups[count])} chars)")
    
    print("=" * 60)


def main():
    """Main function to run the stroke data fetcher."""
    import sys
    
    print("\nüñåÔ∏è  Chinese Character Stroke Data Fetcher")
    print("Collecting data for 100 basic characters for children\n")
    
    # Check for flags
    use_embedded = '--embedded' in sys.argv
    use_individual = '--individual' in sys.argv
    
    stroke_data = []
    
    if use_embedded:
        print("Using embedded dataset (--embedded flag detected)\n")
        stroke_data = create_embedded_dataset()
    else:
        # Fetch individually from CDN (most reliable method)
        print("Fetching characters individually from hanzi-writer CDN...")
        print("This will take a few minutes with rate limiting...\n")
        stroke_data = fetch_all_characters_individually(BASIC_CHARACTERS, delay=0.3)
        
        # Fallback to embedded data if fetch failed
        if not stroke_data:
            print("\n‚ö†Ô∏è  Network fetch failed. Using embedded dataset as fallback...")
            stroke_data = create_embedded_dataset()
    
    if stroke_data:
        # Save standard format
        save_to_json(stroke_data, "chinese_stroke_data.json")
        
        # Save Swift-compatible format
        create_swift_compatible_format(stroke_data, "stroke_data_swift.json")
        
        # Create report
        create_summary_report(stroke_data)
        
        # Show sample
        print("\nüìù Sample data structure:")
        if len(stroke_data) > 0:
            sample = stroke_data[0]
            print(json.dumps({
                'character': sample['character'],
                'unicode': sample['unicode'],
                'stroke_count': sample['stroke_count'],
                'strokes': sample['strokes'][:2] if len(sample['strokes']) > 2 else sample['strokes'],
                'medians': sample['medians'][:2] if len(sample['medians']) > 2 else sample['medians'],
            }, ensure_ascii=False, indent=2))
        
        print("\n‚úÖ Done! Files saved in current directory.")
        print("\nUsage:")
        print("  python3 chinese_stroke_fetcher.py           : Fetch from hanzi-writer CDN")
        print("  python3 chinese_stroke_fetcher.py --embedded : Use built-in stroke counts (offline)")
    else:
        print("\n‚ùå Failed to collect any data.")
        print("Try running with --embedded flag for sample data.")


if __name__ == "__main__":
    main()
